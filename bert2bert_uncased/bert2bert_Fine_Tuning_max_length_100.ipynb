{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "066ERUid44Ee"
      },
      "outputs": [],
      "source": [
        "!pip install transformers --quiet\n",
        "!pip install sentencepiece --quiet\n",
        "!pip install datasets --quiet\n",
        "!pip install evaluate --quiet\n",
        "\n",
        "!pip install git+https://github.com/google-research/bleurt.git -q\n",
        "#!wget -N https://storage.googleapis.com/bleurt-oss-21/BLEURT-20.zip . -q\n",
        "#!unzip -q -n BLEURT-20.zip\n",
        "!wget https://storage.googleapis.com/bleurt-oss-21/BLEURT-20-D12.zip . -q\n",
        "!unzip -q -n BLEURT-20-D12.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cq2qSUAvV6m_"
      },
      "source": [
        "### 2 Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-yU2r1w5O0X"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, load_metric\n",
        "from transformers import BertTokenizer, TFBertModel, BertGenerationEncoder, BertGenerationDecoder, EncoderDecoderModel, GPT2Tokenizer, GPT2Model\n",
        "import evaluate\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "import pandas as pd\n",
        "from csv import writer\n",
        "import math\n",
        "\n",
        "from bleurt import score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zi2vlIaVM-J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13a39918-4112-4c42-c571-018b63c1b05f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# This cell will authenticate you and mount your Drive in the Colab.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWbCb4iK3Rj3"
      },
      "outputs": [],
      "source": [
        "# Load BLEURT\n",
        "bleurt_checkpoint = \"BLEURT-20-D12\"\n",
        "\n",
        "bleurt_metric = score.BleurtScorer(bleurt_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqhK14d4g9Pt"
      },
      "source": [
        "### 3 Data Acquisition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIa6SgpNS643"
      },
      "outputs": [],
      "source": [
        "# select pair languages and min_length\n",
        "orig = \"en\"\n",
        "target = \"zh\"\n",
        "min_length = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_cw8HLdw4lsC"
      },
      "outputs": [],
      "source": [
        "# Data paths\n",
        "train_file = f'drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/{orig}_{target}/train_pairs.csv'\n",
        "val_file = f'drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/{orig}_{target}/val_pairs.csv'\n",
        "test_file = f'drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/{orig}_{target}/test_pairs.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8KlUIzujT1_"
      },
      "outputs": [],
      "source": [
        "# Dictionary to store data sizes\n",
        "data_size = {'en-zh': \n",
        "                {'train': 48444, 'val': 10381},\n",
        "             'en-es':\n",
        "                {'train': 167210, 'val': 35831},\n",
        "             'es-en':\n",
        "                {'train': 167210, 'val': 35831},\n",
        "             'es-zh':\n",
        "                {'train': 45796, 'val': 9814},\n",
        "             'zh-es':\n",
        "                {'train': 45796, 'val': 9814},\n",
        "             'zh-en':\n",
        "                {'train': 48444, 'val': 10381}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kcxvT5CWP10"
      },
      "source": [
        "### 4 Model instantiation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define tokenizer and encoder/decoder\n",
        "model_checkpoint = \"bert-base-multilingual-uncased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "\n",
        "# define sequence to sequence model\n",
        "bert2bert = EncoderDecoderModel.from_encoder_decoder_pretrained(model_checkpoint, model_checkpoint)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUE993hvd9GG",
        "outputId": "6cbfb201-cde3-4f90-a218-f4a2759d4d37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.key.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5 Data Processing"
      ],
      "metadata": {
        "id": "_ysNgS0Xqsa5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taTnEQWN-4iZ"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(text_pair, tokenizer, max_length=100):\n",
        "    orig_text, target_text = text_pair\n",
        "    orig_encoded = tokenizer.batch_encode_plus(\n",
        "        [orig_text],\n",
        "        max_length=max_length,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    orig_input_ids = orig_encoded['input_ids'][0]\n",
        "    orig_attention_mask = orig_encoded['attention_mask'][0]\n",
        "    \n",
        "    target_encoded = tokenizer.batch_encode_plus(\n",
        "        [target_text],\n",
        "        max_length=max_length,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    \n",
        "    target_attention_mask = target_encoded['attention_mask'][0]\n",
        "\n",
        "    label_ids = target_encoded['input_ids'][0]\n",
        "    target_input_ids = label_ids\n",
        "    # We have to make sure that the PAD token is ignored\n",
        "    pad_token_indices = label_ids == tokenizer.pad_token_id\n",
        "    label_ids[pad_token_indices] = -100\n",
        "\n",
        "    target_input_ids = label_ids\n",
        "\n",
        "    \n",
        "    return {'input_ids': orig_input_ids,\n",
        "            'attention_mask': orig_attention_mask,\n",
        "            'labels': label_ids}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_pd49qAX_QL"
      },
      "outputs": [],
      "source": [
        "class TranslationDataIterator:\n",
        "    \n",
        "    def __init__(self,\n",
        "                 tokenizer,\n",
        "                 n_examples,\n",
        "                 max_load_at_once,\n",
        "                 data_filename,\n",
        "                 max_length=100,\n",
        "                 shuffle=True):\n",
        "        \n",
        "        self.tokenizer = tokenizer\n",
        "        self.n_examples = n_examples\n",
        "        self.max_load_at_once = max_load_at_once\n",
        "        self.data_filename = data_filename\n",
        "        self.max_length = max_length\n",
        "        self.shuffle = shuffle\n",
        "        \n",
        "        # Initialize row order, call on_epoch_end to shuffle row indices\n",
        "        self.row_order = np.arange(1, self.n_examples+1)\n",
        "        self.on_epoch_end()\n",
        "\n",
        "        # Load first chunk of max_load_at_once examples\n",
        "        self.df_curr_loaded = self._load_next_chunk(0)\n",
        "        self.curr_idx_in_load = 0\n",
        "    \n",
        "    def _load_next_chunk(self, idx):\n",
        "        load_start = idx\n",
        "        load_end = idx + self.max_load_at_once\n",
        "\n",
        "        # Indices to skip are the ones in the shuffled row_order before and\n",
        "        # after the chunk we'll use for this chunk\n",
        "        load_idx_skip = self.row_order[:load_start] + self.row_order[load_end:]\n",
        "        self.df_curr_loaded = pd.read_csv(self.data_filename, skiprows=load_idx_skip)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.n_examples\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        if self.df_curr_loaded is None or self.curr_idx_in_load >= len(self.df_curr_loaded):\n",
        "            self._load_next_chunk(idx)\n",
        "            self.curr_idx_in_load = 0\n",
        "        \n",
        "        text_pair = self.df_curr_loaded[[f'{orig}', f'{target}']].values.astype(str)[self.curr_idx_in_load]\n",
        "        self.curr_idx_in_load += 1\n",
        "        \n",
        "        item_data = preprocess_data(\n",
        "            text_pair,\n",
        "            self.tokenizer,\n",
        "            self.max_length\n",
        "        )\n",
        "\n",
        "        return item_data\n",
        "    \n",
        "    def __call__(self):\n",
        "        for i in range(self.__len__()):\n",
        "            yield self.__getitem__(i)\n",
        "            \n",
        "            if i == self.__len__()-1:\n",
        "                self.on_epoch_end()\n",
        "    \n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            self.row_order = list(np.random.permutation(self.row_order))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_zUqij_tNvd"
      },
      "outputs": [],
      "source": [
        "# Create the data generators for train and validation data, tensorflow version\n",
        "\n",
        "max_length = 100 \n",
        "max_load_at_once = 1000 \n",
        "\n",
        "train_data_generator = TranslationDataIterator(\n",
        "    tokenizer=tokenizer,\n",
        "    n_examples=data_size[f'{orig}-{target}']['train'],\n",
        "    max_load_at_once=max_load_at_once,\n",
        "    data_filename=train_file,\n",
        "    max_length=max_length\n",
        ")\n",
        "\n",
        "valid_data_generator = TranslationDataIterator(\n",
        "    tokenizer=tokenizer,\n",
        "    n_examples=data_size[f'{orig}-{target}']['val'],\n",
        "    max_load_at_once=max_load_at_once,\n",
        "    data_filename=val_file,\n",
        "    max_length=max_length\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6 Model Training"
      ],
      "metadata": {
        "id": "ZAg0zPNhq2n0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOw9PAZD80Mx"
      },
      "outputs": [],
      "source": [
        "# Specify batch size and other training arguments\n",
        "\n",
        "batch_size = 16 \n",
        "\n",
        "# Modify this filepath to where you want to save the model after fine-tuning\n",
        "if min_length:\n",
        "  dir_path = f'drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/{orig}_{target}/max_length_100/min_length_{min_length}'\n",
        "  file_path = dir_path\n",
        "else:\n",
        "  dir_path = f'drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/{orig}_{target}/max_length_100'\n",
        "  file_path = dir_path\n",
        "\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    file_path,\n",
        "    predict_with_generate=True,\n",
        "    evaluation_strategy='epoch',\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=3,\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlYX1JTwrtm8"
      },
      "outputs": [],
      "source": [
        "# Define metrics\n",
        "def compute_metrics(pred):\n",
        "    labels_ids = pred.label_ids\n",
        "    pred_ids = pred.predictions\n",
        "\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True, max_length = 100)\n",
        "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
        "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True, max_length = 100)\n",
        "    bleurt_score = bleurt_metric.score(references=label_str, candidates=pred_str)\n",
        "\n",
        "    return {\n",
        "        \"bleurt\" : round(bleurt_score[0],4)\n",
        "    }\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZFFqMWDIfiK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea9c93db-d594-4b58-ccd6-74d1c7344e5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50\n"
          ]
        }
      ],
      "source": [
        "# Setting up the special tokens\n",
        "bert2bert.config.decoder_start_token_id = tokenizer.cls_token_id\n",
        "bert2bert.config.eos_token_id = tokenizer.sep_token_id\n",
        "bert2bert.config.pad_token_id = tokenizer.pad_token_id\n",
        "bert2bert.config.vocab_size = bert2bert.config.encoder.vocab_size\n",
        "\n",
        "# Setting up parameters\n",
        "bert2bert.config.max_length = 100\n",
        "\n",
        "if min_length:\n",
        "  bert2bert.config.min_length = min_length\n",
        "else:\n",
        "  pass\n",
        "\n",
        "print(bert2bert.config.min_length)\n",
        "\n",
        "# Define the trainer, passing in the model, training args, and data generators\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model = bert2bert,\n",
        "    compute_metrics=compute_metrics,\n",
        "    args = args,\n",
        "    train_dataset=train_data_generator,\n",
        "    eval_dataset=valid_data_generator\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LV_o67zHBr0F",
        "outputId": "c5f3a305-e960-41c1-d170-8ffaf40ad1db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 48444\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 9084\n",
            "  Number of trainable parameters = 363187095\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9084' max='9084' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [9084/9084 4:54:19, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Bleurt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.912000</td>\n",
              "      <td>2.829103</td>\n",
              "      <td>0.257600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.371100</td>\n",
              "      <td>2.462857</td>\n",
              "      <td>0.446900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.311300</td>\n",
              "      <td>2.344123</td>\n",
              "      <td>0.426200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/en_zh/max_length_100/min_length_50/checkpoint-500\n",
            "Configuration saved in drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/en_zh/max_length_100/min_length_50/checkpoint-500/config.json\n",
            "Model weights saved in drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/en_zh/max_length_100/min_length_50/checkpoint-500/pytorch_model.bin\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/en_zh/max_length_100/min_length_50/checkpoint-1000\n",
            "Configuration saved in drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/en_zh/max_length_100/min_length_50/checkpoint-1000/config.json\n",
            "Model weights saved in drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/en_zh/max_length_100/min_length_50/checkpoint-1000/pytorch_model.bin\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/en_zh/max_length_100/min_length_50/checkpoint-1500\n",
            "Configuration saved in drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/en_zh/max_length_100/min_length_50/checkpoint-1500/config.json\n",
            "Model weights saved in drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/en_zh/max_length_100/min_length_50/checkpoint-1500/pytorch_model.bin\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/en_zh/max_length_100/min_length_50/checkpoint-2000\n",
            "Configuration saved in drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/en_zh/max_length_100/min_length_50/checkpoint-2000/config.json\n",
            "Model weights saved in drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/en_zh/max_length_100/min_length_50/checkpoint-2000/pytorch_model.bin\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/en_zh/max_length_100/min_length_50/checkpoint-2500\n",
            "Configuration saved in drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/en_zh/max_length_100/min_length_50/checkpoint-2500/config.json\n",
            "Model weights saved in drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/en_zh/max_length_100/min_length_50/checkpoint-2500/pytorch_model.bin\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/en_zh/max_length_100/min_length_50/checkpoint-3000\n",
            "Configuration saved in drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/en_zh/max_length_100/min_length_50/checkpoint-3000/config.json\n",
            "Model weights saved in drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/en_zh/max_length_100/min_length_50/checkpoint-3000/pytorch_model.bin\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10381\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/en_zh/max_length_100/min_length_50/checkpoint-3500\n",
            "Configuration saved in drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/en_zh/max_length_100/min_length_50/checkpoint-3500/config.json\n",
            "Model weights saved in drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/en_zh/max_length_100/min_length_50/checkpoint-3500/pytorch_model.bin\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/en_zh/max_length_100/min_length_50/checkpoint-4000\n",
            "Configuration saved in drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/en_zh/max_length_100/min_length_50/checkpoint-4000/config.json\n",
            "Model weights saved in drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/en_zh/max_length_100/min_length_50/checkpoint-4000/pytorch_model.bin\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/en_zh/max_length_100/min_length_50/checkpoint-4500\n",
            "Configuration saved in drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/en_zh/max_length_100/min_length_50/checkpoint-4500/config.json\n",
            "Model weights saved in drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/en_zh/max_length_100/min_length_50/checkpoint-4500/pytorch_model.bin\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/en_zh/max_length_100/min_length_50/checkpoint-5000\n",
            "Configuration saved in drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/en_zh/max_length_100/min_length_50/checkpoint-5000/config.json\n",
            "Model weights saved in drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/en_zh/max_length_100/min_length_50/checkpoint-5000/pytorch_model.bin\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/en_zh/max_length_100/min_length_50/checkpoint-5500\n",
            "Configuration saved in drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/en_zh/max_length_100/min_length_50/checkpoint-5500/config.json\n",
            "Model weights saved in drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/en_zh/max_length_100/min_length_50/checkpoint-5500/pytorch_model.bin\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/en_zh/max_length_100/min_length_50/checkpoint-6000\n",
            "Configuration saved in drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/en_zh/max_length_100/min_length_50/checkpoint-6000/config.json\n",
            "Model weights saved in drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/en_zh/max_length_100/min_length_50/checkpoint-6000/pytorch_model.bin\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10381\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/en_zh/max_length_100/min_length_50/checkpoint-6500\n",
            "Configuration saved in drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/en_zh/max_length_100/min_length_50/checkpoint-6500/config.json\n",
            "Model weights saved in drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/en_zh/max_length_100/min_length_50/checkpoint-6500/pytorch_model.bin\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/en_zh/max_length_100/min_length_50/checkpoint-7000\n",
            "Configuration saved in drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/en_zh/max_length_100/min_length_50/checkpoint-7000/config.json\n",
            "Model weights saved in drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/en_zh/max_length_100/min_length_50/checkpoint-7000/pytorch_model.bin\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/en_zh/max_length_100/min_length_50/checkpoint-7500\n",
            "Configuration saved in drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/en_zh/max_length_100/min_length_50/checkpoint-7500/config.json\n",
            "Model weights saved in drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/en_zh/max_length_100/min_length_50/checkpoint-7500/pytorch_model.bin\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/en_zh/max_length_100/min_length_50/checkpoint-8000\n",
            "Configuration saved in drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/en_zh/max_length_100/min_length_50/checkpoint-8000/config.json\n",
            "Model weights saved in drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/en_zh/max_length_100/min_length_50/checkpoint-8000/pytorch_model.bin\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/en_zh/max_length_100/min_length_50/checkpoint-8500\n",
            "Configuration saved in drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/en_zh/max_length_100/min_length_50/checkpoint-8500/config.json\n",
            "Model weights saved in drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/en_zh/max_length_100/min_length_50/checkpoint-8500/pytorch_model.bin\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/en_zh/max_length_100/min_length_50/checkpoint-9000\n",
            "Configuration saved in drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/en_zh/max_length_100/min_length_50/checkpoint-9000/config.json\n",
            "Model weights saved in drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/en_zh/max_length_100/min_length_50/checkpoint-9000/pytorch_model.bin\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10381\n",
            "  Batch size = 16\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=9084, training_loss=2.745795566990532, metrics={'train_runtime': 17661.538, 'train_samples_per_second': 8.229, 'train_steps_per_second': 0.514, 'total_flos': 1.74195615644424e+16, 'train_loss': 2.745795566990532, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqUNC2EfjU0g"
      },
      "source": [
        "## 5. Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xFt00YfjR2q"
      },
      "outputs": [],
      "source": [
        "test_df = pd.read_csv(test_file)[[f'{orig}', f'{target}']]\n",
        "test_orig= test_df[f'{orig}'].values.astype(str)\n",
        "test_labels = test_df[f'{target}'].values.astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNm20D-ommWY",
        "outputId": "e3b459ec-b5c4-48ef-d5b6-3faa620a4917"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['股市过热恰逢人口趋势不利于退休基金。比如，在德国，20%的人口年龄超过65岁，工作年龄成年人将从今天的5,000万左右下降到2060年的3,400万。而在你新兴市场，预期寿命的快速增长和生育率的下降可能让中国60岁以上人口比例在2050年翻一番——这意味着五亿不事生产的退休者需要支持。',\n",
              "       '然而这么一个世界意味着以色列无法再利用对纳粹大屠杀的愧疚感来影响那些主要势力。那是一个一神论宗教之间的敌意逐渐被多元信仰的汪洋大海所稀释的世界，而在那些仅仅以自身国家利益作为评判标准的，多疑且现实的大国眼中，以色列只能依靠其比较竞争优势来赢得青睐。',\n",
              "       '对美国冠军费舍尔来说，这场对局是他从以神童身份出道以来二十年追求头衔生涯的高潮。对一个超级巨星来说（他甚至经常出现在主要刊物的封面上），费舍尔的生活可谓穷困潦倒，如今，他终于坐在了价值250,000美元的对局边上。当然，这与1971年阿里和弗雷泽（Frazier）拳王争霸战双方都能保证获得的250万美元来说只是九牛一毛。但费舍尔知道，在美国文化中，一切不产生大钱的运动项目都会被边缘化，因此他将这六位数的现金奖金视为国际象棋运动取得进展的终极标志。',\n",
              "       ...,\n",
              "       '在新年来临之际，瑞典通过小步快跑创造了历史，在刚刚结束半年轮换的的外交安全欧盟主席之后，我们将这个职位交给了欧盟新的固定体系 —— 里斯本条约的框架下在布鲁塞尔建立了的体系。',\n",
              "       '像以色列这样缺少强大的人口基础和有利地缘政治条件的小国不可能永久维持占领区，塔尔蒙说。因此，以色列的危险在于徒劳地试图征服巴勒斯坦人。“领导人瞎了眼，看不见前方等待我们的是种族战争。”他写道。',\n",
              "       '例如，联合国难民署帮助解决难民问题；世界粮食计划署为营养不良的儿童提供帮助；而世界卫生组织则支持公共卫生信息系统。这些系统对应对来自于禽流感等流行性疾病的威胁至关重要。联合国没有资源解决如艾滋病或全球气候变化等新问题，但它在敦促各国政府采取行动方面可以起到重要的召集者的作用。'],\n",
              "      dtype='<U471')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "test_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBNljsJkUeEU"
      },
      "outputs": [],
      "source": [
        "# Upload saved fine-tuned model\n",
        "dir_path = f'drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/{orig}_{target}'\n",
        "file_path = f'{dir_path}'\n",
        "bert2bert_saved = EncoderDecoderModel.from_pretrained(file_path + '/checkpoint-9000')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_examples = 100\n",
        "start_index = 0\n",
        "end_index = num_examples\n",
        "test_size = len(test_orig)\n",
        "num_batches = math.ceil(test_size/num_examples)\n",
        "test_bleurt_scores_file = f'drive/MyDrive/MIDS/W266/Final_Project/bert2bert-finetuned/{orig}_{target}/test_bleurt_scores.csv'\n"
      ],
      "metadata": {
        "id": "JIP-PIj_TqIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(test_bleurt_scores_file, 'a') as f_object:\n",
        "  # and get a writer object\n",
        "  writer_object = writer(f_object)\n",
        "  \n",
        "  for _ in range(num_batches): \n",
        "      # Get predictions\n",
        "      test_input_ids = tokenizer.batch_encode_plus(test_orig[start_index: end_index], return_tensors=\"pt\", padding=True, truncation=True, max_length=100)\n",
        "      test_output_token_ids = bert2bert_saved.generate(test_input_ids.input_ids)\n",
        "      test_decoded = tokenizer.batch_decode(test_output_token_ids, skip_special_tokens=True, \n",
        "                                  clean_up_tokenization_spaces=False, max_length = 100)\n",
        "\n",
        "      # Compute Bleurt scores\n",
        "      bleurt_scores = bleurt_metric.score(references = test_labels[start_index: end_index], candidates = test_decoded)\n",
        "\n",
        "      # pass the list as an argument into writerow()\n",
        "      writer_object.writerow(bleurt_scores)\n",
        "\n",
        "      # update indices\n",
        "      start_index = end_index\n",
        "\n",
        "      if end_index + num_examples > test_size:\n",
        "        end_index = test_size\n",
        "      else:\n",
        "        end_index += num_examples\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaHRgH9Ze2x9",
        "outputId": "68d61083-a7bc-4267-c521-1d5857735f4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py:1364: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 100 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  UserWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mebV8UcpVrZ"
      },
      "outputs": [],
      "source": [
        "test_input_ids = tokenizer.batch_encode_plus(test_orig[:5], return_tensors=\"pt\", padding=True, truncation=True, max_length=50)\n",
        "test_output_token_ids = bert2bert_saved.generate(test_input_ids.input_ids)\n",
        "test_decoded = tokenizer.batch_decode(test_output_token_ids)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[tokenizer.decode(out_ids, skip_special_tokens=True, \n",
        "                               clean_up_tokenization_spaces=False, max_length = 50) for out_ids in test_output_token_ids]"
      ],
      "metadata": {
        "id": "jLq1WP8h_Huk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rE00BwXAMxDp"
      },
      "outputs": [],
      "source": [
        "test_output_token_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCl7MwEA1IB5"
      },
      "outputs": [],
      "source": [
        "test_decoded"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}